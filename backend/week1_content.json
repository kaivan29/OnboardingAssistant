{"week_number":1,"reading_material":{"title":"LSM Fundamentals and Setup Mastery","content":"# Week 1: LSM Fundamentals and Setup Mastery\n\nWelcome to your first week onboarding into RocksDB, the battle-hardened key-value store powering high-performance workloads at Meta, Apple, and beyond. This week, you'll demystify the Log-Structured Merge-Tree (LSM-tree) architecture, master local setup, and write your first client code. By the end, you'll benchmark at 1M+ ops/sec, trace data flows, and grasp why LSMs trade write amplification for read efficiency—like a cosmic dance between sequential writes and inevitable merges.\n\nThink of RocksDB as a digital filing cabinet optimized for frenzy: writes fly in sequentially, but reads demand surgical precision across tiers of sorted files. We'll cover theory, hands-on builds, API mastery, and codebase dives into `util/`, `port/`, `file/`, `logging/`, and `memtable/`.\n\n## 1. LSM-Tree Theory: The Heart of RocksDB\n\nKey-value stores like RocksDB solve the \"write-fast, read-fast\" dilemma using LSM-trees, pioneered by Patrick O'Neil's 1996 paper and popularized by LevelDB (2011). RocksDB (2012+) extends it with leveled compaction, column families, and compression.\n\n### Core Components\n- **Write-Ahead Log (WAL)**: Durability king. Every `Put`/`Delete` appends to a segmented log on disk before hitting memory.\n- **Memtables**: In-memory sorted buffers (default: SkipList). Writes are O(log N) inserts.\n- **Immutable Memtables**: When full, memtables \"freeze\" and flush to disk.\n- **SSTables**: Sorted String Tables—immutable, append-only files in levels (L0 to Lmax). L0 is unsorted by file; higher levels are sorted and overlap-minimized.\n\n### ASCII Diagram: LSM Write Path\n```\nAppends (Writes) -->\n  WAL (disk) + Active Memtable (RAM)\n                |\n             (Full? Flush!)\n                v\nImmutable Memtable -->\n  Sorted --> SSTable (L0)\n                |\n           Background Compaction\n                v\nL0 (overlapping) --> L1 (non-overlapping, ~10x size) --> ... --> Lmax\nReads: Merge Memtable + Snapshots + SSTables (Bloom-filtered)\n```\n\n### Write/Read Amplification Trade-offs\nWrites are cheap: sequential appends to WAL/memtables (~100-500KB/sec disk-limited). But flushes/compactions amplify writes—data rewrites multiple times (write amp: 10-100x). Why? **Physics invariants**: HDDs/SSDs crave sequential I/O; random writes kill throughput (seek time ~10ms). LSM amortizes this via batching/merging.\n\nReads amplify too: merge K memtables + V SSTables (read amp: 10-20x worst-case), but Bloom filters (false positive ~1%) skip 90%+ files. Analogy: Library card catalog (index blocks) + Dewey decimals (sorted SSTs) + RFID skips (Bloom).\n\n**Relating to Physics**: Conservation of I/O—sequential writes violate no \"energy\" (disk head moves linearly), but random reads do (quadratic seeks). Compaction restores order, like entropy in thermodynamics.\n\n**LevelDB/RocksDB Papers**:\n- LevelDB: Log-structured with leveling (size amp 10x per level).\n- RocksDB: Adds universal compaction (FIFO-like), FIFO for logs.\n\n## 2. Build System Mastery: CMake, Buck, and Dependencies\n\nRocksDB's tech stack: C++17, CMake (primary), Buck (Meta-scale). Dependencies: lz4/zstd/snappy (compression), jemalloc (alloc), CRC32c (checksums).\n\n### Step-by-Step Local Build (CMake)\n1. Clone: `git clone https://github.com/facebook/rocksdb.git && cd rocksdb`\n2. Deps (Ubuntu): `sudo apt install liblz4-dev libzstd-dev libsnappy-dev`\n3. Build: `cmake -DCMAKE_BUILD_TYPE=Release -DWITH_TESTS=OFF . && make -j$(nproc)`\n   - Tune: `-DROCKSDB_BUILD_PARALLEL=1` for perf.\n4. Buck alt: `./buck build //:rocksdb` (needs Buck installed).\n\n**Hands-On: db_bench**\nCompile-built `db_bench` is your perf playground.\n```\n./db_bench --benchmarks=fillseq --num=1000000 --value_size=100 --threads=1 \\\n  --db=/tmp/rocksdb_test --use_existing_db=0\n```\nOutput: Seq fill ~1M ops/sec on SSD. Tune for 1M+ reads/writes:\n- `--block_size=16k` (balance cache/I/O).\n- `--compression_type=lz4`.\n- `--bloom_bits=10`.\n- `--write_buffer_size=64M` (bigger memtable = fewer flushes).\n\nPro Tip: `--benchmarks=fillseq,fillrseq,seekrandom` reveals amp.\n\n## 3. Public API: Your KV Client Toolkit\n\nExposed in `include/rocksdb/db.h`. RAII-heavy (Status returns errors idiomatically).\n\n### Core Classes\n- `DB*`: `rocksdb::DB::Open(options, path)`.\n- `WriteOptions`, `ReadOptions`.\n- `WriteBatch`: Atomic multi-ops.\n- `Iterator`: Forward/reverse scans.\n\n**Hands-On: Implement Simple KV Client**\nCompile/run `examples/simple.cc`:\n```cpp\n#include <rocksdb/db.h>\n\nint main() {\n  rocksdb::DB* db;\n  rocksdb::Options options;\n  options.create_if_missing = true;\n  rocksdb::Status s = rocksdb::DB::Open(options, \"/tmp/testdb\", &db);\n  assert(s.ok());\n\n  // Put/Get\n  s = db->Put(rocksdb::WriteOptions(), \"key1\", \"value1\");\n  std::string value;\n  s = db->Get(rocksdb::ReadOptions(), \"key1\", &value);  // value == \"value1\"\n\n  // WriteBatch\n  rocksdb::WriteBatch batch;\n  batch.Put(\"key2\", \"value2\");\n  batch.Delete(\"key1\");\n  s = db->Write(rocksdb::WriteOptions(), &batch);\n\n  // Iterator\n  rocksdb::Iterator* it = db->NewIterator(rocksdb::ReadOptions());\n  for (it->SeekToFirst(); it->Valid(); it->Next()) {\n    printf(\"%s: %s\\n\", it->key().ToString().c_str(), it->value().ToString().c_str());\n  }\n  delete it;\n  delete db;\n  return 0;\n}\n```\nBuild: `g++ simple.cc -lrocksdb -o simple -O3 -std=c++17`\nDebug: Watch `INFO` logs (e.g., memtable full → flush).\n\n**Trace Writes: WAL → Memtable → SSTable**\n1. `Put(k,v)`: Serialize to WAL (`file/writable_file_writer.cc`), insert SkipList (`memtable/skiplist.h`).\n2. Memtable full (`write_buffer_size`): → Immutable (`db/db_impl.cc:FlushMemTable`).\n3. Background flusher: Sort → Block builder (`table/block_based_table_builder.cc`) → L0 SST.\nLogs: Enable `options.info_log_level = rocksdb::InfoLogLevel::INFO_LEVEL`.\n\n## 4. Focus Areas: Low-to-Medium Complexity Dirs\n\n### Low Complexity: Setup Utils\n- **`util/`**: Arenas (bump allocators, `util/arena.h`), CRC32c (`util/crc32c.cc`), histograms (`util/histogram.cc`). RAII everywhere: `Status::Status()`.\n- **`port/`**: POSIX threads (`port/posix/port_posix.cc`), atomics.\n- **`file/`**: `WritableFile` (direct I/O, `file/writable_file_writer.cc`), `RandomAccessFile`.\n- **`logging/`**: `Logger` (`logging/event_logger.cc`), `INFO()` macros.\n\nBrowse: `grep -r 'Arena' util/`\n\n### Medium: `memtable/` – SkipList Deep Dive\nDefault `skiplist.h`: Lock-free, probabilistic O(log N) SkipList (Pugh 1990). Nodes: `forward[]` arrays for +1/+4/+16 jumps.\n\n**Key Code: Insert/Lookup**\n```cpp\n// skiplist.h (simplified)\nstruct Node {\n  std::string key;\n  std::string value;\n  Node* forward[4];  // Max height 4 (prob 1/4)\n};\n\nvoid Insert(const Slice& key, const Slice& value) {\n  // Random height, traverse down, insert links atomically.\n}\n\nbool Get(const LookupKey& k, std::string* value, Status* s) {\n  Node* x = head_->forward[0];\n  // Skip down levels.\n}\n```\n`memtable.cc`: `MemTable::MemTable(const InternalKeyComparator& cmp, MemTableRep* rep)` wraps SkipListRep.\n\n**Why SkipList?** Faster than tree (cache-friendly), simpler than B+tree.\n\n## 5. Hands-On Challenges\n1. Tune `db_bench` for 1M+ QPS: Vary `block_cache_size=1G`, measure amp (`--statistics`).\n2. Debug client: Force flush (`db->Flush(rocksdb::FlushOptions())`), ls `/tmp/testdb/` SSTs.\n3. Trace in GDB: `gdb --args ./db_bench ...`, `b DBImpl::Put`, step WAL write.\n4. Mod `simple.cc`: Add 1M bulk `WriteBatch`, benchmark.\n\n## Summary & Next Steps\nYou've built the foundation: LSM physics, blazing builds, API fluency, memtable mastery. Words amplified like SSTables—now merge into muscle memory. Next week: SSTables & compaction.\n\n(Word count: ~1750)","key_concepts":["LSM-tree architecture (WAL, memtables, SSTables, levels)","Write/read amplification and sequential I/O physics","RocksDB public API (DB::Open, Put/Get/Delete, WriteBatch, Iterator)","Build with CMake/Buck, dependencies (lz4/zstd), db_bench tuning","Memtable SkipList (insertions, lookups, probabilistic heights)","Low-complexity dirs: util/arena, port/POSIX, file I/O, logging","Write trace: WAL append → memtable insert → immutable flush → L0 SST"],"resources":["LevelDB Documentation (original LSM inspiration): https://github.com/google/leveldb/blob/master/doc/index.md","RocksDB Official Site & Blog: https://rocksdb.org/blog/","RocksDB Tuning Guide: https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide","Build Instructions: https://github.com/facebook/rocksdb/blob/main/README.md","LSM-Tree Original Paper (O'Neil): https://www.usenix.org/legacy/publications/library/proceedings/fast/v6_2_2/v6_2_2-o_neil.pdf","SkipList Paper (Pugh): https://research.swtch.com/skip","db_bench Usage Examples: https://github.com/facebook/rocksdb/wiki/db_bench-Manual"],"reason":null},"coding_tasks":[{"id":"task-1","title":"Build RocksDB and Extend examples/simple.cc for Basic CRUD","description":"Clone the RocksDB repo, build it using CMake (cmake .. && make -j), and compile/run examples/simple.cc. Copy it to examples/my_simple.cc and modify it to: 1) Open a DB with default Options. 2) Perform 1000 sequential PUTs (keys 'key0' to 'key999', values same as keys). 3) Use an Iterator to scan all keys and verify values match. 4) Add timing with std::chrono to measure total PUT time. Output ops/sec. Ensure Status::OK() checks everywhere.","difficulty":"easy","estimated_time":"45 mins - 1 hour","files_to_modify":["examples/my_simple.cc"],"hints":["Use DB::Open(Options(), \"/tmp/testdb\", &db).","Disable WAL with Options().disableWAL = true; for faster testing initially.","Iterator: it->SeekToFirst(); while(it->Valid()) { ... it->Next(); }","Include <chrono>: auto start = std::chrono::steady_clock::now(); ... double secs = duration.count(); ops_sec = 1000 / secs;"],"reason":null},{"id":"task-2","title":"Implement WriteBatch for Batched Writes and Performance Comparison","description":"Extend examples/my_simple.cc: 1) Add a function to do 10,000 single PUTs, time it. 2) Add another function using WriteBatch: batch 1000 puts per Write(db->Write(write_options, &batch)), repeat 10 times, time it. 3) Output throughput (ops/sec) for both, expect batch ~2-5x faster. 4) Verify data with Iterator after each method. Clean up DB between runs.","difficulty":"medium","estimated_time":"1 - 1.5 hours","files_to_modify":["examples/my_simple.cc"],"hints":["WriteBatch batch; for(int i=0; i<1000; i++) batch.Put(key, value); db->Write(WriteOptions(), &batch); batch.Clear();","Use the same keys range but offset for second test to avoid overwrites.","Set WriteOptions().sync = false; for max speed.","Compare to single puts: loop db->Put(...); observe memtable usage via logs."],"reason":null},{"id":"task-3","title":"Benchmark 1M Sequential Writes with Basic Tuning","description":"Create a new file tools/my_bench.cc (model after db_bench). Open DB with tuned Options: write_buffer_size=64<<20 (64MB), max_write_buffer_number=4, level0_file_num_compaction_trigger=4. Perform fillseq benchmark: 1M PUTs ('key000000'..'key999999'). Time total writes, compute writes/sec. Run db_bench --benchmarks=fillseq --num=1000000 --use_existing_db=false for comparison. Output both throughputs; aim for >500k ops/sec writes.","difficulty":"medium","estimated_time":"1.5 - 2 hours","files_to_modify":["tools/my_bench.cc"],"hints":["Build with make tools/my_bench (add to CMakeLists.txt if needed).","Tune on SSD; use /tmp/testdb. Format keys as printf(\"key%06d\", i).","Options tuning reduces flush/compaction stalls (relates to LSM write amp).","Parse args with rocksdb::Env::Default()->GetHostName() or simple argc check."],"reason":null},{"id":"task-4","title":"Trace Writes from WAL/Memtable to SSTables with Logging and Snapshots","description":"Extend tools/my_bench.cc: 1) Enable verbose logging: Options().info_log_level = rocksdb::InfoLogLevel::INFO_LEVEL; 2) After 1M fillseq PUTs, create a ReadOnly snapshot (const Snapshot* snap = db->GetSnapshot()). 3) Perform random GETs using snapshot (ReadOptions.read_snapshot = snap). 4) Force flush: db->Flush(FlushOptions()). 5) Run again, observe logs for WAL appends, memtable->immutable, SSTable creation (Level 0). Release snapshot, explain trade-offs in comments.","difficulty":"medium-hard","estimated_time":"2 hours","files_to_modify":["tools/my_bench.cc"],"hints":["Logs show 'Write #xx memtable', 'Flushing memtable', 'Level-0 SST'.","Snapshots pin versions, increasing read amp (multi-level merges).","Relate to LSM: writes O(1) append, reads O(log N) due to levels/merges.","Use db_bench --benchmarks=fillseq --num=100000 --log_max_logfiles=10 for log observation."],"reason":null}],"quiz":[{"id":"q1","question":"What is the primary trade-off that LSM-trees like those in RocksDB make to achieve high write throughput?","options":["A) Lower storage usage at the cost of slower reads","B) Higher write amplification via compactions in exchange for sequential writes and efficient reads","C) Random writes for faster point lookups","D) Eliminating the need for indexes entirely"],"correct_answer":1,"explanation":"LSM-trees prioritize fast sequential writes to WAL and memtables, accepting write amplification from later compactions/merges to maintain read efficiency across sorted SSTables, relating to the 'physics invariant' of sequential IO being much faster than random IO on disks."},{"id":"q2","question":"In RocksDB's write path, when you call DB::Put(key, value), where is the data first written for durability?","options":["A) Directly to SSTables in Level 0","B) To the Write-Ahead Log (WAL) and then inserted into the memtable","C) Only to the memtable, WAL is optional","D) To an iterator buffer"],"correct_answer":1,"explanation":"Writes go first to the WAL for crash recovery (durability), then to the in-memory memtable. This enables fast append-only writes before eventual flushing to SSTables."},{"id":"q3","question":"What triggers a memtable to flush into an immutable memtable and eventually an SSTable in RocksDB?","options":["A) Every individual Put operation","B) Reaching a configurable size threshold (e.g., write_buffer_size) or explicit flush","C) Only during reads for on-demand sorting","D) Automatically after every compaction"],"correct_answer":1,"explanation":"Memtables are mutable in-memory structures (default: SkipList). When full, they become immutable and flush to disk as sorted SSTables, preventing unbounded memory growth and enabling background compactions."},{"id":"q4","question":"RocksDB's default memtable implementation uses which data structure for efficient sorted lookups and range scans?","options":["A) std::unordered_map for O(1) average lookups","B) SkipList for logarithmic-time insertions and ordered traversal","C) std::vector for sequential access only","D) B+-tree for balanced random access"],"correct_answer":1,"explanation":"The SkipList (skiplist.h, memtable.cc) provides probabilistic O(log n) operations while staying cache-friendly and simpler than trees, ideal for memtable's sorted key-value pairs."},{"id":"q5","question":"Which RocksDB public API feature allows batching multiple Put, Delete, or Merge operations for atomicity and efficiency?","options":["A) Iterator for scanning keys","B) WriteBatch for grouping operations into a single write","C) DB::Open for database initialization","D) WAL for recovery only"],"correct_answer":1,"explanation":"WriteBatch reduces overhead by amortizing WAL writes and memtable updates across multiple ops, applied atomically on Write() call—key for high-throughput clients like examples/simple.cc."},{"id":"q6","question":"To trace and benchmark RocksDB performance locally aiming for 1M+ ops/sec, which tool and benchmark would you primarily use?","options":["A) CMake configure --benchmarks=fillseq","B) db_bench --benchmarks=fillseq (after building with dependencies like lz4/zstd)","C) examples/simple.cc --trace","D) SkipList unit tests"],"correct_answer":1,"explanation":"db_bench is RocksDB's microbenchmark suite. --benchmarks=fillseq populates the DB sequentially (deterministic keys), allowing tuning (e.g., block_cache_size, compression) to hit 1M+ ops/sec on modern hardware."},{"id":"q7","question":"In a simple KV client using RocksDB's public API (e.g., examples/simple.cc), what is the correct sequence to perform basic operations?","options":["A) Iterator::Seek(), DB::Put(), DB::Open()","B) DB::Open(), DB::Put()/Get()/Delete(), then DB::Close()","C) WriteBatch::Put(), DB::Open(), Iterator::Valid()","D) DB::Delete() before DB::Open()"],"correct_answer":1,"explanation":"Standard flow: Open a DB handle (DB::Open), perform CRUD ops (Put/Get/Delete or WriteBatch), iterate if needed (Iterator), and close. This tests practical API usage and debugging."},{"id":"q8","question":"Why do LSM-trees relate to 'physics invariants' in RocksDB's design, particularly for write/read trade-offs?","options":["A) Electrons in SSDs prefer random writes","B) Sequential IO (writes to WAL/SSTables) is orders of magnitude faster than random IO due to disk/SSD platter mechanics","C) Compactions violate conservation of data","D) Memtables ignore flash endurance limits"],"correct_answer":1,"explanation":"Physics of storage (HDD seek times, SSD parallelism) favors sequential appends/merges over random B-tree updates, enabling LSM's high write rates despite amplification."},{"id":"q9","question":"During debugging a client or tracing writes, which RocksDB directories contain low-complexity utilities for porting, files, and logging?","options":["A) memtable/, table/, db/","B) util/, port/, file/, logging/","C) cache/, compaction/, bloom/","D) options/, snapshot/, merge/"],"correct_answer":1,"explanation":"These form the setup foundation: util/port for cross-platform, file for IO abstractions, logging for traces—crucial for building/running examples and understanding data flow without deep dives."}]}